{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKi3Hj5tbMnu"
      },
      "source": [
        "## Start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzwBCnY-bMYv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyO869YBg0b0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install pandas datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_VjiZIDg6Q-"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import and Load Data\n",
        "import pandas as pd\n",
        "import re\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load your CSV from Google Drive or upload manually\n",
        "file_path = \"/content/drive/MyDrive/Kenya_Quantised_LLM/data/train.csv\"  # Replace with your path or use file upload\n",
        "t_file_path = \"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test.csv\"  # Replace with your path or use file upload\n",
        "raw_df = pd.read_csv(file_path)\n",
        "t_raw_df = pd.read_csv(t_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoqRUQbkhPM1"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define Cleaning & Formatting Function\n",
        "def restructure_prompt(prompt):\n",
        "    # Remove self-introduction\n",
        "    prompt = re.sub(r\"^I am a nurse.*?in Kenya\\.\\s*\", \"\", prompt, flags=re.DOTALL)\n",
        "\n",
        "    # Split by 'Questions:' or 'Questions' block\n",
        "    if \"Questions:\" in prompt:\n",
        "        parts = prompt.split(\"Questions:\", maxsplit=1)\n",
        "    elif \"Questions\" in prompt:\n",
        "        parts = prompt.split(\"Questions\", maxsplit=1)\n",
        "    else:\n",
        "        parts = [prompt, \"\"]\n",
        "\n",
        "    vignette = parts[0].strip()\n",
        "    questions = parts[1].strip()\n",
        "\n",
        "    return (\n",
        "        \"<|system|>\\n\"\n",
        "        \"You are a highly experienced Kenyan clinical nurse. Provide clear, concise, and empathetic answers.\\n\"\n",
        "        \"<|user|>\\n\"\n",
        "        f\"{vignette}\\n\\nQuestions:\\n{questions}\\n\"\n",
        "        \"<|assistant|>\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AbLcLYfhYhL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 4: Apply to Dataset\n",
        "raw_df[\"input_text\"] = raw_df[\"Prompt\"].apply(restructure_prompt)\n",
        "raw_df[\"output_text\"] = raw_df[\"Clinician\"]\n",
        "\n",
        "t_raw_df[\"input_text\"] = t_raw_df[\"Prompt\"].apply(restructure_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtMzKyT6hglz"
      },
      "outputs": [],
      "source": [
        "# Step 5: Create HuggingFace Dataset for Fine-tuning\n",
        "hf_dataset = Dataset.from_pandas(raw_df[[\"input_text\", \"output_text\"]])\n",
        "hf_dataset.to_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/finetune_dataset.csv\")\n",
        "hf_t_dataset = Dataset.from_pandas(t_raw_df[[\"input_text\"]])\n",
        "hf_t_dataset.to_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IQiJCk_iH54"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_4gZkY5hlOh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install transformers peft bitsandbytes accelerate datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYW7UBbehrpY"
      },
      "outputs": [],
      "source": [
        "# Step 2: Imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwhw5nMAiMHL"
      },
      "outputs": [],
      "source": [
        "# Step 3: Load Local Fine-tune and Test Datasets\n",
        "train_path = \"/content/drive/MyDrive/Kenya_Quantised_LLM/data/finetune_dataset.csv\"\n",
        "test_path = \"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test_dataset.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "train_dataset_full = Dataset.from_pandas(train_df[[\"input_text\", \"output_text\"]])\n",
        "test_dataset = Dataset.from_pandas(test_df[[\"input_text\"]])\n",
        "\n",
        "# Split train into train/validation\n",
        "train_test_split = train_dataset_full.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itg1Pq6_iQVo"
      },
      "outputs": [],
      "source": [
        "# Step 4: Load Tokenizer and Model (TinyLLaMA example)\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD5auxbfidoU"
      },
      "outputs": [],
      "source": [
        "# Step 5: Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Cq-tKvifXY"
      },
      "outputs": [],
      "source": [
        "# Step 6: Tokenize\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        text_target=example[\"output_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize, batched=True)\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__PFnDADig7w"
      },
      "outputs": [],
      "source": [
        "# Step 7: TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"qlora-tinyllama-clinician\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbg6CP_Tiioe"
      },
      "outputs": [],
      "source": [
        "# Step 8: Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmX7nHU7ikht"
      },
      "outputs": [],
      "source": [
        "# Step 9: Save Final Model\n",
        "model.save_pretrained(\"qlora-tinyllama-clinician-final\")\n",
        "tokenizer.save_pretrained(\"qlora-tinyllama-clinician-final\")\n",
        "print(\"âœ… Fine-tuned model saved to qlora-tinyllama-clinician-final\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeMEzSiGbPLx"
      },
      "source": [
        "## Basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW37jg5_bbK_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVOj2MUZbRLA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L21zYuQrvUz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAN-S9E8bWIj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yClirtccQME"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/train.csv\")\n",
        "df.rename({\"Prompt\": \"input_text\", \"Clinician\": \"output_text\"}, inplace=True, axis=1)\n",
        "df = df[[\"input_text\", \"output_text\"]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe-Mz8-jciP8"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrmESMhbdAj-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or your custom quantized model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FodvcA5enQm"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bw5zYSnezVU"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        text_target=example[\"output_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osV_7rblfA0Y"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWw6xmC_hzzK"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-gr1GI1hzl4"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test.csv\")\n",
        "\n",
        "test_df.rename({\"Prompt\": \"input_text\"}, inplace=True, axis=1)\n",
        "\n",
        "def tokenize_for_inference(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=None  # Must return dict, not tensors\n",
        "    )\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_for_inference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "umrRbBe1hDUj"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "generated_output = []\n",
        "\n",
        "for example in tqdm(tokenized_test_dataset):\n",
        "    input_ids = tokenizer(example[\"input_text\"], return_tensors=\"pt\", truncation=True, max_length=256).input_ids.to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids = input_ids,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    generated_output.append(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6lxpUk3ijVm"
      },
      "outputs": [],
      "source": [
        "generated_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Ce9LJmk033"
      },
      "source": [
        "## Trial 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR6c4PppjTBb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sgNpGjxflJrP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/train_raw.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "feHTROHdlBGA"
      },
      "outputs": [],
      "source": [
        "def restructure_prompt(prompt):\n",
        "    import re\n",
        "\n",
        "    # Step 1: Match the nurse intro sentence (up to first period after \"Kenya\")\n",
        "    match = re.match(r\"(I am a nurse.*?in .*?county in Kenya\\.)\\s*(.*)\", prompt, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        system_msg = match.group(1).strip()\n",
        "        main_prompt = match.group(2).strip()\n",
        "    else:\n",
        "        # Fallback if pattern isn't found\n",
        "        system_msg = \"You are a highly experienced Kenyan clinical nurse.\"\n",
        "        main_prompt = prompt.strip()\n",
        "\n",
        "    return (\n",
        "        f\"### SYSTEM\\n{system_msg}\\n\\n\"\n",
        "        f\"### PROMPT\\n{main_prompt}\\n\\n\"\n",
        "        f\"### RESPONSE\\n\"\n",
        "    )\n",
        "\n",
        "df[\"input_text\"] = df[\"Prompt\"].apply(restructure_prompt)\n",
        "df[\"output_text\"] = df[\"Clinician\"]\n",
        "\n",
        "df = df[[\"input_text\", \"output_text\"]]\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zbkECZglYps"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(f\"case {i}: \\n{df['input_text'][i]}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Rbkfc4leKe"
      },
      "outputs": [],
      "source": [
        "dataset= Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset= dataset[\"train\"]\n",
        "test_dataset= dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idJbZVUdonmm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or your custom quantized model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCSuJu7spX0U"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs6pX3eDsDCa"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        text_target=example[\"output_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebVExNuEsIWA"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kklTQUZt3T-"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK7gS4SvsUcU"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test.csv\")\n",
        "\n",
        "test_df.rename({\"Prompt\": \"input_text\"}, inplace=True, axis=1)\n",
        "\n",
        "def tokenize_for_inference(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=None  # Must return dict, not tensors\n",
        "    )\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_for_inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDEazILnt4yM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "generated_output = []\n",
        "\n",
        "for example in tqdm(tokenized_test_dataset):\n",
        "    input_ids = tokenizer(example[\"input_text\"], return_tensors=\"pt\", truncation=True, max_length=256).input_ids.to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids = input_ids,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    generated_output.append(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU83318ft7mw"
      },
      "outputs": [],
      "source": [
        "generated_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys2cmkNOuWrV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial 4"
      ],
      "metadata": {
        "id": "AbAt-LIaOPlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl transformers datasets peft accelerate bitsandbytes evaluate --quiet"
      ],
      "metadata": {
        "id": "NKZqEyVBOQid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/train_raw.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "17mUnNZNN_mS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restructure_prompt(prompt):\n",
        "    import re\n",
        "\n",
        "    # Step 1: Match the nurse intro sentence (up to first period after \"Kenya\")\n",
        "    match = re.match(r\"(I am a nurse.*?in .*?county in Kenya\\.)\\s*(.*)\", prompt, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        system_msg = match.group(1).strip()\n",
        "        main_prompt = match.group(2).strip()\n",
        "    else:\n",
        "        # Fallback if pattern isn't found\n",
        "        system_msg = \"You are a highly experienced Kenyan clinical nurse.\"\n",
        "        main_prompt = prompt.strip()\n",
        "\n",
        "    return (\n",
        "        f\"### SYSTEM\\n{system_msg}\\n\\n\"\n",
        "        f\"### PROMPT\\n{main_prompt}\\n\\n\"\n",
        "        f\"### RESPONSE\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "df[\"input_text\"] = df[\"Prompt\"].apply(restructure_prompt)\n",
        "df[\"output_text\"] = df[\"Clinician\"]\n",
        "\n",
        "df = df[[\"input_text\", \"output_text\"]]\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "5GWvr1K3ObRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset= dataset[\"train\"]\n",
        "eval_dataset= dataset[\"test\"]"
      ],
      "metadata": {
        "id": "is7SxDZMObMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or your custom quantized model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZqFUWPoJObJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SFTT trainer with QLoRA\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_dir=\"./logs\",\n",
        "    num_train_epochs=15,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "FCmYJtTSObGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score --quiet"
      ],
      "metadata": {
        "id": "BdY6WYYHUCoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PnGEhx_ObDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args,\n",
        "    formatting_func=lambda ex: f\"{ex['input_text']}{ex['output_text']}\",\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "J7ypT-EFSH2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "ChJViygx6Dbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/test.csv\")\n",
        "# test_df.rename(columns={\"Prompt\": \"input_text\"}, inplace=True)"
      ],
      "metadata": {
        "id": "wjUsFzOossrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "test_df[\"input_text\"] = test_df[\"Prompt\"].apply(restructure_prompt)\n"
      ],
      "metadata": {
        "id": "CwWWMo7LUMSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def format_prompt(row):\n",
        "    prompt = row.get(\"input_text\", \"\").strip()\n",
        "    competency = row.get(\"Nursing Competency\", \"general practice\")\n",
        "    panel = row.get(\"Clinical Panel\", \"general clinical care\")\n",
        "\n",
        "    match = re.match(r\"(I am a nurse.*?in .*?county in Kenya\\.)\\s*(.*)\", prompt, flags=re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        system_intro = match.group(1).strip()\n",
        "        main_prompt = match.group(2).strip()\n",
        "    else:\n",
        "        system_intro = \"You are a highly experienced Kenyan clinical nurse.\"\n",
        "        main_prompt = prompt.strip()\n",
        "\n",
        "    system_message = (\n",
        "        f\"{system_intro} I specialize in '{competency}' and I am to provide a response for the '{panel}' clinical panel.\"\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        f\"### SYSTEM\\n{system_message}. Provide clear, structured, and comprehensive answers. Include diagnosis, recommended investigations, and step-by-step management.\\n\\n\"\n",
        "        f\"### PROMPT\\n{main_prompt}\\n\\n\"\n",
        "        f\"### RESPONSE\\n\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Ju3e9HQBsY0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "\n",
        "generated_output = []\n",
        "\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    formatted_prompt = format_prompt(row)\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=250,\n",
        "            do_sample=True,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    generated_output.append(output_text)\n"
      ],
      "metadata": {
        "id": "vDLQllb1tTlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " generated_output"
      ],
      "metadata": {
        "id": "c0WNxYRxtZu3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"Generated_Response\"] = generated_output\n",
        "# test_df.to_csv(\"test_predictions.csv\", index=False)\n",
        "\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "E-OhJJrAxor0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_response(text):\n",
        "    if \"### RESPONSE\" in text:\n",
        "        return text.split(\"### RESPONSE\", 1)[-1].strip()\n",
        "    else:\n",
        "        return text.strip()  # fallback if not formatted\n",
        "\n",
        "test_df[\"Cleaned_Response\"] = test_df[\"Generated_Response\"].apply(extract_response)\n"
      ],
      "metadata": {
        "id": "0jhzoL_mzDSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.rename({\"Cleaned_Response\": \"Clinician\"}, inplace=True, axis=1)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "PYfOcbAnyZZS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(f\"case {i}: ###CASE\\n{test_df['input_text'][i]}\\n###RESPONSE\\n{test_df['Clinician'][i]}\\n\\n\")"
      ],
      "metadata": {
        "id": "e-nlSQ-Ex5Lr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[[\"Master_Index\", \"Clinician\"]].to_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/submissions/qlora_submission_1.csv\", index=False)"
      ],
      "metadata": {
        "id": "q0f3P6qA0bSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kg5ewr7R1p3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial 5"
      ],
      "metadata": {
        "id": "eBOUCJ_M580e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl transformers datasets peft accelerate bitsandbytes evaluate --quiet"
      ],
      "metadata": {
        "id": "4ejFs0yA5-Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Kenya_Quantised_LLM/data/train_raw.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mdTVrRdoQX4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restructure_prompt(prompt):\n",
        "    import re\n",
        "\n",
        "    # Step 1: Match the nurse intro sentence (up to first period after \"Kenya\")\n",
        "    match = re.match(r\"(I am a nurse.*?in .*?county in Kenya\\.)\\s*(.*)\", prompt, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        system_msg = match.group(1).strip()\n",
        "        main_prompt = match.group(2).strip()\n",
        "    else:\n",
        "        # Fallback if pattern isn't found\n",
        "        system_msg = \"You are a highly experienced Kenyan clinical nurse.\"\n",
        "        main_prompt = prompt.strip()\n",
        "\n",
        "    return (\n",
        "        f\"### SYSTEM\\n{system_msg}. Ensure you match the diagnosis with symptoms\\n\\n\"\n",
        "        f\"### PROMPT\\n{main_prompt}\\n\\n\"\n",
        "        f\"### RESPONSE\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "df[\"input_text\"] = df[\"Prompt\"].apply(restructure_prompt)\n",
        "df[\"output_text\"] = df[\"Clinician\"]\n",
        "\n",
        "df = df[[\"input_text\", \"output_text\"]]\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "M_SrvMInA0o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(f\"case {i}: \\n{df['input_text'][i]}\\n\\n\")"
      ],
      "metadata": {
        "id": "9T4VZfQhPaC2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset= dataset[\"train\"]\n",
        "eval_dataset= dataset[\"test\"]"
      ],
      "metadata": {
        "id": "Ee8FTZTGI-Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or your custom quantized model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "k2OJLkL3Pc7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SFTT trainer with QLoRA\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        "    bf16=True  # if supported\n",
        ")"
      ],
      "metadata": {
        "id": "HYBJ8gLJPf3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "fFO4GyNyIeJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip and align\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE\n",
        "    results = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Optional: Round results\n",
        "    results = {k: round(v, 4) for k, v in results.items()}\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "8D6vpk_9Pihj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_example(example):\n",
        "    return tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        text_target=example[\"output_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Apply to both train and eval\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_example, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_example, batched=True)"
      ],
      "metadata": {
        "id": "yt_AfN94KiMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "Fs7n3RjZMyoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "class CustomSFTTrainer(SFTTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Shift so that tokens <n> predict <n+1>\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "            ignore_index=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "ybavd2VxNmBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with SFTTrainer\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = CustomSFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics  # Optional\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ya2JY7u7PllH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GH-d_PUIkw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "pKi3Hj5tbMnu",
        "6IQiJCk_iH54",
        "yeMEzSiGbPLx",
        "s9Ce9LJmk033"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}